{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10058605,"sourceType":"datasetVersion","datasetId":6198327},{"sourceId":10103266,"sourceType":"datasetVersion","datasetId":6231823},{"sourceId":10120764,"sourceType":"datasetVersion","datasetId":6244948},{"sourceId":10142297,"sourceType":"datasetVersion","datasetId":6260089}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["qVolP0vg_PQv","zqUAQGgX_TD-","VAVbWsks-ukh","jPAVplQ3_iTB","DRZmL1sFtEu6","s8p6QfgL-ukl","cCUHw_1DtEu6","-Mj-x4FytEu7","gWrzNJAvtEu7","NQ9ADLjOtEu8"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Setup notebook"],"metadata":{"id":"qVolP0vg_PQv"}},{"cell_type":"code","source":["import os\n","import sys\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import keras\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import RandomTranslation\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.layers import Add, Concatenate, Dense, GlobalAveragePooling2D, Multiply, Lambda\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Dense, LayerNormalization, MultiHeadAttention, Dropout, Reshape, Input\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Layer\n","import random\n","%matplotlib inline\n","\n","\n","seed = 42\n","random.seed(seed)  # Imposta il seed per il modulo random\n","np.random.seed(seed)  # Imposta il seed per numpy\n","tf.random.set_seed(seed)\n","\n","BATCH_SIZE = 64\n","batch_size=64\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","tf.get_logger().setLevel('ERROR')\n","sys.stderr = open(os.devnull, 'w')"],"metadata":{"trusted":true,"id":"fQg004zotEuw","execution":{"iopub.status.busy":"2024-12-09T14:05:39.287248Z","iopub.execute_input":"2024-12-09T14:05:39.287752Z","iopub.status.idle":"2024-12-09T14:05:51.405825Z","shell.execute_reply.started":"2024-12-09T14:05:39.287714Z","shell.execute_reply":"2024-12-09T14:05:51.405233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Load data and augmentation"],"metadata":{"id":"zqUAQGgX_TD-"}},{"cell_type":"code","source":["training_data = np.load(\"/kaggle/input/mars-for-students-cleaned-compressed/mars_for_students_cleaned_compressed.npz\")\n","images = training_data[\"images\"]/255\n","labels = training_data[\"labels\"]"],"metadata":{"trusted":true,"id":"q_B9MxpDtEux","execution":{"iopub.status.busy":"2024-12-09T14:05:51.407616Z","iopub.execute_input":"2024-12-09T14:05:51.408552Z","iopub.status.idle":"2024-12-09T14:05:52.354260Z","shell.execute_reply.started":"2024-12-09T14:05:51.408506Z","shell.execute_reply":"2024-12-09T14:05:52.353694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2)"],"metadata":{"trusted":true,"id":"UP46392otEux","execution":{"iopub.status.busy":"2024-12-09T14:05:52.355353Z","iopub.execute_input":"2024-12-09T14:05:52.355632Z","iopub.status.idle":"2024-12-09T14:05:52.462103Z","shell.execute_reply.started":"2024-12-09T14:05:52.355605Z","shell.execute_reply":"2024-12-09T14:05:52.461354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"add_channel\"\n","def add_channel(image, label):\n","    image = tf.cast(image, tf.float32)\n","    label = tf.cast(label, tf.float32)\n","    image = tf.expand_dims(image, axis=-1)\n","    label = tf.expand_dims(label, axis=-1)\n","    return image, label\n","\n","def add_channel_lambda(dataset):\n","    return dataset.map(\n","                        lambda x, y: add_channel(x, y),\n","                        num_parallel_calls=tf.data.AUTOTUNE\n","                       )"],"metadata":{"trusted":true,"id":"vTgteY7RtEuy","execution":{"iopub.status.busy":"2024-12-09T14:05:52.464020Z","iopub.execute_input":"2024-12-09T14:05:52.464324Z","iopub.status.idle":"2024-12-09T14:05:52.469247Z","shell.execute_reply.started":"2024-12-09T14:05:52.464297Z","shell.execute_reply":"2024-12-09T14:05:52.468620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"Flip Left Right\"\n","@tf.function\n","def random_flip_left_right(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (tf.image.flip_left_right(image), tf.image.flip_left_right(label)),\n","        lambda: (image, label)\n","    )\n","\n","    return image, label\n","\n","def random_flip_left_right_lambda(dataset, thr):\n","    return dataset.map(\n","                        lambda x, y: random_flip_left_right(x, y, thr),\n","                        num_parallel_calls=tf.data.AUTOTUNE\n","                       )"],"metadata":{"trusted":true,"id":"FQ2MOOqGtEuy","execution":{"iopub.status.busy":"2024-12-09T14:05:52.470195Z","iopub.execute_input":"2024-12-09T14:05:52.470468Z","iopub.status.idle":"2024-12-09T14:05:52.481626Z","shell.execute_reply.started":"2024-12-09T14:05:52.470442Z","shell.execute_reply":"2024-12-09T14:05:52.481093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"Flip Up Down\"\n","@tf.function\n","def random_flip_up_down(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (tf.image.flip_up_down(image), tf.image.flip_up_down(label)),\n","        lambda: (image, label)\n","    )\n","\n","    return image, label\n","\n","def random_flip_up_down_lambda(dataset, thr):\n","    return dataset.map(\n","                        lambda x, y: random_flip_up_down(x, y, thr),\n","                        num_parallel_calls=tf.data.AUTOTUNE\n","                       )"],"metadata":{"trusted":true,"id":"tWGovm9gtEuy","execution":{"iopub.status.busy":"2024-12-09T14:05:52.482515Z","iopub.execute_input":"2024-12-09T14:05:52.482724Z","iopub.status.idle":"2024-12-09T14:05:52.495537Z","shell.execute_reply.started":"2024-12-09T14:05:52.482703Z","shell.execute_reply":"2024-12-09T14:05:52.494979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"Translation\"\n","@tf.function\n","def translation(image, label):\n","    max_translation = 0.5\n","\n","    # Dimensioni dell'immagine\n","    height = tf.shape(image)[0]\n","    width = tf.shape(image)[1]\n","\n","    # Calcolo massimo spostamento in base alla percentuale specificata\n","    max_dx = tf.cast(max_translation * tf.cast(width, tf.float32), tf.int32)\n","    max_dy = tf.cast(max_translation * tf.cast(height, tf.float32), tf.int32)\n","\n","    # Generazione spostamenti casuali\n","    dx = tf.random.uniform([], -max_dx, max_dx + 1, dtype=tf.int32)\n","    dy = tf.random.uniform([], -max_dy, max_dy + 1, dtype=tf.int32)\n","\n","    # Funzione per lo shift con riempimento a zero\n","    def zero_fill_translation():\n","        white_image = tf.ones((64, 128, 1), dtype=tf.float32)\n","        padded_image = tf.image.pad_to_bounding_box(white_image, max_dy, max_dx, height + 2 * max_dy, width + 2 * max_dx)\n","        cropped_image = tf.image.crop_to_bounding_box(padded_image, max_dy - dy, max_dx - dx, height, width)\n","        padded_label = tf.image.pad_to_bounding_box(white_image, max_dy, max_dx, height + 2 * max_dy, width + 2 * max_dx)\n","        cropped_label = tf.image.crop_to_bounding_box(padded_label, max_dy - dy, max_dx - dx, height, width)\n","        return image*cropped_image, label*cropped_label\n","\n","    # Funzione per lo shift con riflessione\n","    def reflection_translation():\n","        translated_image = tf.roll(image, shift=[dy, dx], axis=[0, 1])\n","        translated_label = tf.roll(label, shift=[dy, dx], axis=[0, 1])\n","        return translated_image, translated_label\n","\n","    # Decide casualmente tra riflessione o riempimento di zeri\n","    use_reflection = tf.random.uniform([], 0, 1) < 0.8\n","    translated_image, translated_label = tf.cond(\n","        use_reflection,\n","        true_fn=reflection_translation,\n","        false_fn=zero_fill_translation\n","    )\n","\n","    return translated_image, translated_label\n","\n","\n","@tf.function\n","def random_translation(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (translation(image, label)),\n","        lambda: (image, label)\n","    )\n","    return image, label\n","\n","def translation_lambda(dataset, thr):\n","    return dataset.map(\n","                        lambda x, y: random_translation(x, y, thr),\n","                        num_parallel_calls=tf.data.AUTOTUNE\n","                       )"],"metadata":{"trusted":true,"id":"hFler4TCtEuz","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:52.496486Z","iopub.execute_input":"2024-12-09T14:05:52.496719Z","iopub.status.idle":"2024-12-09T14:05:52.511726Z","shell.execute_reply.started":"2024-12-09T14:05:52.496697Z","shell.execute_reply":"2024-12-09T14:05:52.511143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"zoom\"\n","@tf.function\n","def zoom(image, label, zoom_range=(0.8, 1.2)):\n","    # Genera un fattore di zoom casuale\n","    zoom_factor = tf.random.uniform([], zoom_range[0], zoom_range[1])\n","\n","    # Ottieni le dimensioni originali\n","    original_height = tf.shape(image)[0]\n","    original_width = tf.shape(image)[1]\n","\n","    # Calcola le nuove dimensioni dopo lo zoom\n","    new_height = tf.cast(tf.cast(original_height, tf.float32) * zoom_factor, tf.int32)\n","    new_width = tf.cast(tf.cast(original_width, tf.float32) * zoom_factor, tf.int32)\n","\n","    # Ridimensiona l'immagine e la maschera\n","    zoomed_image = tf.image.resize(image, [new_height, new_width], method='bilinear')\n","    zoomed_label = tf.image.resize(label, [new_height, new_width], method='bilinear')  # Per maschere, meglio 'nearest'\n","\n","    # Ritaglia o pad per riportare alle dimensioni originali\n","    cropped_image = tf.image.resize_with_crop_or_pad(zoomed_image, original_height, original_width)\n","    cropped_label = tf.image.resize_with_crop_or_pad(zoomed_label, original_height, original_width)\n","\n","    return cropped_image, cropped_label\n","\n","@tf.function\n","def random_zoom(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (zoom(image, label)),\n","        lambda: (image, label)\n","    )\n","    return image, label\n","\n","def zoom_lambda(dataset, thr):\n","    return dataset.map(\n","        lambda x, y: random_zoom(x, y, thr),\n","        num_parallel_calls=tf.data.AUTOTUNE\n","    )\n"],"metadata":{"trusted":true,"id":"23e_kCcvtEuz","execution":{"iopub.status.busy":"2024-12-09T14:05:52.512762Z","iopub.execute_input":"2024-12-09T14:05:52.513018Z","iopub.status.idle":"2024-12-09T14:05:52.528475Z","shell.execute_reply.started":"2024-12-09T14:05:52.512977Z","shell.execute_reply":"2024-12-09T14:05:52.527878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"Cutout\"\n","@tf.function\n","def cutout(image, label):\n","    # Calcola altezza e larghezza dell'immagine\n","    height = tf.shape(image)[0]\n","    width = tf.shape(image)[1]\n","\n","\n","    # Genera la dimensione del quadrato\n","    square_size = tf.random.uniform([], 15, 48 + 1, dtype=tf.int32)\n","\n","    # Genera la posizione del quadrato\n","    max_x = tf.maximum(0, width - square_size)\n","    max_y = tf.maximum(0, height - square_size)\n","\n","    start_x = tf.random.uniform([], 0, max_x + 1, dtype=tf.int32)\n","    start_y = tf.random.uniform([], 0, max_y + 1, dtype=tf.int32)\n","\n","    # Crea una maschera per \"cancellare\" il quadrato\n","    mask = tf.ones_like(image)\n","    mask = tf.tensor_scatter_nd_update(\n","        mask,\n","        indices=tf.stack(tf.meshgrid(\n","            tf.range(start_y, start_y + square_size),\n","            tf.range(start_x, start_x + square_size)\n","        ), axis=-1),\n","        updates=tf.zeros([square_size, square_size, tf.shape(image)[-1]], dtype=image.dtype)\n","    )\n","\n","    # Applica la maschera a immagine e label\n","    zeroed_image = image * mask\n","    zeroed_label = label * mask\n","\n","    return zeroed_image, zeroed_label\n","\n","@tf.function\n","def random_cutout(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: cutout(image,label),\n","        lambda: (image, label)\n","    )\n","    return image, label\n","\n","def cutout_lambda(dataset, thr):\n","    return dataset.map(\n","        lambda x, y: random_cutout(x, y, thr),\n","        num_parallel_calls=tf.data.AUTOTUNE\n","    )\n","\n"],"metadata":{"trusted":true,"id":"KK1x2PuNtEu0","execution":{"iopub.status.busy":"2024-12-09T14:05:52.529541Z","iopub.execute_input":"2024-12-09T14:05:52.529861Z","iopub.status.idle":"2024-12-09T14:05:52.545460Z","shell.execute_reply.started":"2024-12-09T14:05:52.529827Z","shell.execute_reply":"2024-12-09T14:05:52.544867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"contrast\"\n","@tf.function\n","def random_contrast(image, label, thr):\n","    prob = tf.random.uniform([], seed=seed)\n","    factor = tf.random.uniform([], minval=-4, maxval=4, seed=seed)\n","\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (tf.image.adjust_contrast(image, factor), label),\n","        lambda: (image, label)\n","    )\n","    return image, label\n","\n","def contrast_lambda(dataset, thr):\n","    return dataset.map(\n","        lambda x, y: random_contrast(x, y, thr),\n","        num_parallel_calls=tf.data.AUTOTUNE\n","    )"],"metadata":{"trusted":true,"id":"akTg7k8rtEu0","execution":{"iopub.status.busy":"2024-12-09T14:05:52.548087Z","iopub.execute_input":"2024-12-09T14:05:52.548382Z","iopub.status.idle":"2024-12-09T14:05:52.560962Z","shell.execute_reply.started":"2024-12-09T14:05:52.548356Z","shell.execute_reply":"2024-12-09T14:05:52.560422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"rotate\"\n","@tf.function\n","def rotate(image, label, max_angle=30):\n","    \"\"\"\n","    Ruota l'immagine e la sua maschera di un angolo casuale tra -max_angle e +max_angle gradi.\n","\n","    Parametri:\n","    - image: Tensor, immagine di input (64, 128, 1).\n","    - label: Tensor, maschera associata (64, 128, 1).\n","    - max_angle: int, angolo massimo di rotazione (default 30 gradi).\n","\n","    Ritorna:\n","    - image, label: immagine e maschera ruotate.\n","    \"\"\"\n","    # Genera un angolo casuale tra -max_angle e +max_angle (in gradi)\n","    angle = tf.random.uniform([], minval=-max_angle, maxval=max_angle, dtype=tf.float32)\n","\n","    # Converte l'angolo in radianti\n","    angle_rad = angle * tf.constant(np.pi / 180, dtype=tf.float32)  # Converte in radianti\n","\n","    # Crea la matrice di trasformazione affine per la rotazione\n","    rotation_matrix = tf.stack([\n","        tf.cos(angle_rad), -tf.sin(angle_rad), 0.0,\n","        tf.sin(angle_rad), tf.cos(angle_rad), 0.0,\n","        0.0, 0.0, 1.0\n","    ])\n","\n","    # Applica la trasformazione affine all'immagine e alla maschera\n","    image = tf.image.transform(image, rotation_matrix, interpolation='BILINEAR')\n","    label = tf.image.transform(label, rotation_matrix, interpolation='NEAREST')\n","\n","    return image, label\n","\n","@tf.function\n","def random_rotate(image, label, thr):\n","    prob = tf.random.uniform([])\n","\n","    image, label = tf.cond(\n","        prob < thr,\n","        lambda: (rotate(image, label)),\n","        lambda: (image, label)\n","    )\n","    return image, label\n","\n","\n","def rotate_lambda(dataset, thr):\n","    \"\"\"\n","    Applica la rotazione casuale continua a tutte le immagini e maschere di un dataset.\n","\n","    Parametri:\n","    - dataset: tf.data.Dataset, dataset da trasformare.\n","    - max_angle: int, angolo massimo di rotazione (default 30 gradi).\n","\n","    Ritorna:\n","    - Dataset con rotazione casuale applicata.\n","    \"\"\"\n","    return dataset.map(\n","        lambda x, y: random_rotate(x, y, thr),\n","        num_parallel_calls=tf.data.AUTOTUNE\n","    )\n"],"metadata":{"trusted":true,"id":"jX5NZi1gtEu0","execution":{"iopub.status.busy":"2024-12-09T14:05:52.561895Z","iopub.execute_input":"2024-12-09T14:05:52.562176Z","iopub.status.idle":"2024-12-09T14:05:52.572929Z","shell.execute_reply.started":"2024-12-09T14:05:52.562146Z","shell.execute_reply":"2024-12-09T14:05:52.572322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"multiply\"\n","def multiply_dataset(images, labels, quantity = 1):\n","        indices = [i for i in range(len(labels)) if 4 in labels[i]]\n","\n","        # Seleziona le immagini e le etichette corrispondenti\n","        images2 = images[indices]\n","        labels2 = labels[indices]\n","\n","        images = np.concatenate([images, images[:quantity], images[:quantity], images[:quantity], images2, images2 ], axis=0)\n","        labels = np.concatenate([labels, labels[:quantity], labels[:quantity], labels[:quantity], labels2, labels2 ], axis=0)\n","        return (images, labels)"],"metadata":{"trusted":true,"id":"7q8SxrT_tEu0","execution":{"iopub.status.busy":"2024-12-09T14:05:52.573779Z","iopub.execute_input":"2024-12-09T14:05:52.574055Z","iopub.status.idle":"2024-12-09T14:05:52.588487Z","shell.execute_reply.started":"2024-12-09T14:05:52.574031Z","shell.execute_reply":"2024-12-09T14:05:52.587909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\"cutmix\"\n","def paste_patch(image, label, patch_image, patch_label, patch_size=(32, 32)):\n","    \"\"\"Incolla una porzione (patch) di un'altra immagine e label nella posizione casuale dell'immagine corrente.\"\"\"\n","    h, w = patch_size\n","    img_h, img_w = image.shape[:2]\n","\n","    # Definisci la posizione casuale dove incollare la patch\n","    top = np.random.randint(0, img_h - h, seed = seed)\n","    left = np.random.randint(0, img_w - w, seed = seed)\n","\n","    # Incolla la patch sull'immagine\n","    image[top:top+h, left:left+w] = patch_image[:h, :w]\n","    label[top:top+h, left:left+w] = patch_label[:h, :w]\n","\n","    return image, label\n","\n","def data_augmentation_fn(image, label, batch_idx, patch_size=(32, 32), frequency=2):\n","    \"\"\"Applica l'operazione di incolla una patch ogni 'frequency' batch.\"\"\"\n","    if batch_idx % frequency == 0:\n","        # Seleziona una porzione casuale da un'altra immagine del dataset\n","        idx = np.random.randint(0, len(X_train))  # Scegli un'altra immagine casuale dal dataset\n","        patch_image = X_train[idx]\n","        patch_label = y_train[idx]\n","\n","        # Incolla la patch sull'immagine e sulla label\n","        image, label = paste_patch(image, label, patch_image, patch_label, patch_size)\n","\n","    return image, label\n","\n","\n","\n","\n","def cutmix_dataset(X_train, y_train):\n","    # Estrai indici casuali senza ripetizione\n","    indices = np.random.choice(len(X_train), size=len(X_train), replace=False)\n","\n","    # Crea un array per le immagini e le etichette aggiornate\n","    X_cutmix = np.copy(X_train)\n","    y_cutmix = np.copy(y_train)\n","\n","    # Impostiamo la dimensione della porzione da tagliare e incollare (ad esempio 50x50 pixel)\n","    height, width = X_train.shape[1:]  # dimensioni delle immagini\n","    cut_size = 50  # altezza e larghezza della porzione da incollare\n","\n","    for i in range(len(X_train)):\n","        # Selezioniamo una porzione casuale di dimensioni cut_size x cut_size in X_random[i]\n","        x_offset = np.random.randint(0, width - cut_size)\n","        y_offset = np.random.randint(0, height - cut_size)\n","\n","        # Estrai la porzione dall'immagine X_random[i]\n","        X_patch = X_train[indices[i], y_offset:y_offset+cut_size, x_offset:x_offset+cut_size]\n","        y_patch = y_train[indices[i], y_offset:y_offset+cut_size, x_offset:x_offset+cut_size]\n","\n","        # Incolla la porzione nell'immagine X_train[i]\n","        X_cutmix[i, y_offset:y_offset+cut_size, x_offset:x_offset+cut_size] = X_patch\n","        y_cutmix[i, y_offset:y_offset+cut_size, x_offset:x_offset+cut_size] = y_patch\n","    return X_cutmix, y_cutmix"],"metadata":{"trusted":true,"id":"vuMUidG1tEu1","execution":{"iopub.status.busy":"2024-12-09T14:05:52.589767Z","iopub.execute_input":"2024-12-09T14:05:52.590097Z","iopub.status.idle":"2024-12-09T14:05:52.605799Z","shell.execute_reply.started":"2024-12-09T14:05:52.590062Z","shell.execute_reply":"2024-12-09T14:05:52.605164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def to_dataset(X_train, y_train, augmentation = False, shuffle = False, batch_size = BATCH_SIZE, duplicate = False, cycles = 1, strange = True):\n","    if duplicate:\n","        X_train, y_train = multiply_dataset(X_train, y_train, len(X_train))\n","    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=batch_size * 2)\n","\n","    dataset = add_channel_lambda(dataset)\n","\n","    if augmentation:\n","        if strange:\n","            dataset = cutout_lambda(dataset,1)\n","        dataset = random_flip_left_right_lambda(dataset,0.5)\n","        dataset = random_flip_up_down_lambda(dataset,0.5)\n","        dataset = translation_lambda(dataset,0.5)\n","        dataset = zoom_lambda(dataset,0.5)\n","\n","\n","    dataset = dataset.batch(batch_size, drop_remainder=False)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","    return dataset"],"metadata":{"trusted":true,"id":"emNBnY9ttEu1","execution":{"iopub.status.busy":"2024-12-09T14:05:52.606775Z","iopub.execute_input":"2024-12-09T14:05:52.606987Z","iopub.status.idle":"2024-12-09T14:05:52.621479Z","shell.execute_reply.started":"2024-12-09T14:05:52.606966Z","shell.execute_reply":"2024-12-09T14:05:52.620866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["train_dataset = to_dataset(X_train, y_train, augmentation = True, duplicate = True, strange = False)\n","val_dataset = to_dataset(X_val, y_val, augmentation = False, duplicate = True, strange = False)"],"metadata":{"trusted":true,"id":"h7qmRwjZtEu3","execution":{"iopub.status.busy":"2024-12-09T14:05:52.622764Z","iopub.execute_input":"2024-12-09T14:05:52.623034Z","iopub.status.idle":"2024-12-09T14:05:58.143140Z","shell.execute_reply.started":"2024-12-09T14:05:52.623010Z","shell.execute_reply":"2024-12-09T14:05:58.142595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"VAVbWsks-ukh"}},{"cell_type":"code","source":["num_classes = 5\n","epoch = 1000\n","patience = 50"],"metadata":{"trusted":true,"id":"hkSELkO4tEu3","execution":{"iopub.status.busy":"2024-12-09T14:05:58.144231Z","iopub.execute_input":"2024-12-09T14:05:58.144529Z","iopub.status.idle":"2024-12-09T14:05:58.148021Z","shell.execute_reply.started":"2024-12-09T14:05:58.144503Z","shell.execute_reply":"2024-12-09T14:05:58.147127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def aspp_block(input_tensor, filters, name=\"ASPP\"):\n","\n","    # Convoluzione 1x1\n","    conv1 = layers.Conv2D(filters, (1, 1), padding=\"same\", activation=\"relu\", name=f\"{name}_conv1\")(input_tensor)\n","\n","    # Convoluzioni dilatate (3x3) con diversi tassi di dilatazione\n","    conv3_1 = layers.Conv2D(filters, (3, 3), dilation_rate=1, padding=\"same\", activation=\"relu\", name=f\"{name}_conv3_6\")(input_tensor)\n","    conv3_2 = layers.Conv2D(filters, (3, 3), dilation_rate=4, padding=\"same\", activation=\"relu\", name=f\"{name}_conv3_12\")(input_tensor)\n","    conv3_3 = layers.Conv2D(filters, (3, 3), dilation_rate=8, padding=\"same\", activation=\"relu\", name=f\"{name}_conv3_18\")(input_tensor)\n","\n","    # Global Average Pooling\n","    global_avg = layers.GlobalAveragePooling2D(name=f\"{name}_gap\")(input_tensor)\n","    global_avg = layers.Reshape((1, 1, input_tensor.shape[-1]), name=f\"{name}_reshape\")(global_avg)\n","    global_avg = layers.Conv2D(filters, (1, 1), padding=\"same\", activation=\"relu\", name=f\"{name}_conv_global\")(global_avg)\n","    global_avg = layers.UpSampling2D(size=(input_tensor.shape[1], input_tensor.shape[2]), interpolation=\"bilinear\", name=f\"{name}_upsample\")(global_avg)\n","\n","    # Concatenazione\n","    x = layers.Concatenate(name=f\"{name}_concat\")([conv1, conv3_1, conv3_2, conv3_3, global_avg])\n","\n","    # Convoluzione Finale per Ridurre i Canali\n","    x = layers.Conv2D(filters, (1, 1), padding=\"same\", activation=\"relu\", name=f\"{name}_conv_out\")(x)\n","\n","    x = layers.UpSampling2D((2, 2))(x)\n","\n","    return x\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.149180Z","iopub.execute_input":"2024-12-09T14:05:58.149537Z","iopub.status.idle":"2024-12-09T14:05:58.168722Z","shell.execute_reply.started":"2024-12-09T14:05:58.149500Z","shell.execute_reply":"2024-12-09T14:05:58.168192Z"},"id":"Qkde8ZFF-ukj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from tensorflow.keras.saving import register_keras_serializable\n","@register_keras_serializable(package=\"Custom\", name=\"TransformerBottleneck\")\n","class TransformerBottleneck(Layer):\n","    def __init__(self, num_heads=4, projection_dim=128, ff_dim=256, dropout_rate=0.1,\n","                width= 2,\n","                height=4,\n","                channel=512,\n","                batch_size = batch_size,\n","            **kwargs):\n","        super(TransformerBottleneck, self).__init__()\n","        self.num_heads = num_heads\n","        self.projection_dim = projection_dim\n","        self.ff_dim = ff_dim\n","        self.dropout_rate = dropout_rate\n","\n","        # Definisci i componenti del Transformer\n","        self.layer_norm1 = LayerNormalization(epsilon=1e-6)\n","        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate)\n","        self.layer_norm2 = LayerNormalization(epsilon=1e-6)\n","        self.dense_ff1 = Dense(ff_dim, activation=\"relu\")\n","        self.dense_ff2 = Dense(projection_dim)\n","        self.layer_norm3 = LayerNormalization(epsilon=1e-6)\n","\n","        self.width = width\n","        self.height = height\n","        self.channel = channel\n","        self.batch_size = batch_size\n","\n","    def call(self, inputs):\n","        # Ottieni le dimensioni per il reshaping\n","        num_patches = self.height * self.width\n","\n","        # Appiattisci il tensore\n","        #reshaped_input = tf.reshape(inputs, (self.batch_size, num_patches, self.channel))\n","\n","        # Applicazione del Transformer\n","        norm1 = self.layer_norm1(inputs)\n","        attention_output = self.multi_head_attention(norm1, norm1)\n","        add1 = Add()([inputs, attention_output])\n","        norm2 = self.layer_norm2(add1)\n","\n","        ff1 = self.dense_ff1(norm2)\n","        ff2 = self.dense_ff2(ff1)\n","        add2 = Add()([add1, ff2])\n","        output_tensor = self.layer_norm3(add2)\n","\n","        # Ripristina la forma originale\n","        #output_tensor = tf.reshape(output_tensor, (self.batch_size, self.width, self.height, self.channel))\n","\n","        return output_tensor\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"num_heads\": self.num_heads,\n","            \"projection_dim\": self.projection_dim,\n","            \"ff_dim\": self.ff_dim,\n","            \"width\": self.width,\n","            \"height\": self.height,\n","            \"channel\": self.channel,\n","        })\n","        return config"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.170042Z","iopub.execute_input":"2024-12-09T14:05:58.170382Z","iopub.status.idle":"2024-12-09T14:05:58.185016Z","shell.execute_reply.started":"2024-12-09T14:05:58.170346Z","shell.execute_reply":"2024-12-09T14:05:58.184443Z"},"id":"b_3WqvUw-ukj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def gated_skip_connection(x, skip, mode=\"add\", gating=\"learnable\", name=\"gated_connection\"):\n","    if gating == \"learnable\":\n","        # Learnable scalars for gating\n","        alpha = tf.Variable(0.5, trainable=True, name=f\"{name}_alpha\")\n","        beta = tf.Variable(0.5, trainable=True, name=f\"{name}_beta\")\n","\n","        # Normalize weights with a softmax\n","        alpha, beta = tf.nn.softmax([alpha, beta], axis=0)\n","        if mode == \"add\":\n","            output = Add(name=f\"{name}_add\")([alpha * x, beta * skip])\n","        elif mode == \"concat\":\n","            output = Concatenate(name=f\"{name}_concat\")([alpha * x, beta * skip])\n","        else:\n","            raise ValueError(\"Unsupported mode. Use 'add' or 'concat'.\")\n","\n","    elif gating == \"dynamic\":\n","        # Dynamic gating based on skip and decoder features\n","        combined = Concatenate(name=f\"{name}_dynamic_concat\")([x, skip])\n","        gate = GlobalAveragePooling2D(name=f\"{name}_gap\")(combined)\n","        gate = Dense(1, activation=\"sigmoid\", name=f\"{name}_gate\")(gate)\n","\n","        # Apply gating\n","        gate = Lambda(lambda z: tf.expand_dims(tf.expand_dims(z, axis=1), axis=1), name=f\"{name}_expand\")(gate)\n","        gated_skip = Multiply(name=f\"{name}_multiply\")([skip, gate])\n","        output = Add(name=f\"{name}_add_dynamic\")([x, gated_skip])\n","\n","    else:\n","        raise ValueError(\"Unsupported gating mode. Use 'learnable' or 'dynamic'.\")\n","\n","    return output\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.186003Z","iopub.execute_input":"2024-12-09T14:05:58.186259Z","iopub.status.idle":"2024-12-09T14:05:58.202453Z","shell.execute_reply.started":"2024-12-09T14:05:58.186234Z","shell.execute_reply":"2024-12-09T14:05:58.201875Z"},"id":"rAORkPX--ukj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def network1(input_shape=(64, 128, 1), num_classes=5, filter_factor=0.5):\n","\n","    inputs = layers.Input(shape=input_shape)\n","\n","    # Blocco 1\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", name=\"block1_conv1\")(inputs)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block1_act1\")(x)\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", name=\"block1_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block1_act2\")(x)\n","    block1 = x  # Skip connection per U-Net\n","    x = layers.MaxPooling2D((2, 2), strides=2, name=\"block1_pool\")(x)  # (32, 64, 64)\n","\n","    # Blocco 2\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", name=\"block2_conv1\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block2_act1\")(x)\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", name=\"block2_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block2_act2\")(x)\n","    block2 = x  # Skip connection per U-Net\n","    x = layers.MaxPooling2D((2, 2), strides=2, name=\"block2_pool\")(x)  # (16, 32, 128)\n","\n","    # Blocco 3\n","    x = layers.Conv2D(256, (3, 3), padding=\"same\", name=\"block3_conv1\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block3_act1\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout1\")(x)\n","    x = layers.Conv2D(256, (3, 3), padding=\"same\", name=\"block3_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block3_act2\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout2\")(x)\n","    x = layers.Conv2D(256, (3, 3), padding=\"same\", name=\"block3_conv3\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block3_act3\")(x)\n","    block3 = x  # Skip connection per U-Net\n","    x = layers.MaxPooling2D((2, 2), strides=2, name=\"block3_pool\")(x)  # (8, 16, 256)\n","\n","    # Blocco 4\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block4_conv1\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block4_act1\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout3\")(x)\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block4_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block4_act2\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout4\")(x)\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block4_conv3\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block4_act3\")(x)\n","    block4 = x  # Skip connection per U-Net\n","    x = layers.MaxPooling2D((2, 2), strides=2, name=\"block4_pool\")(x)  # (4, 8, 512)\n","\n","    # Blocco 5\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block5_conv1\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block5_act1\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout5\")(x)\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block5_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block5_act2\")(x)\n","    x = layers.Dropout(0.3, name=\"decoder_network1_dropout6\")(x)\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"block5_conv3\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"block5_act3\")(x)\n","    block5 = x  # Skip connection per U-Net\n","    x = layers.MaxPooling2D((2, 2), strides=2, name=\"block5_pool\")(x)  # (2, 4, 512)\n","\n","    # Stampe per debug delle dimensioni\n","    print(\"Dimensioni di block1:\", block1.shape)\n","    print(\"Dimensioni di block2:\", block2.shape)\n","    print(\"Dimensioni di block3:\", block3.shape)\n","    print(\"Dimensioni di block4:\", block4.shape)\n","    print(\"Dimensioni di block5:\", block5.shape)\n","\n","    reshaped = Reshape((x.shape[1] * x.shape[2], x.shape[3]))(x)\n","    bottleneck = TransformerBottleneck(num_heads=4, projection_dim=512, ff_dim=256 ,width= x.shape[1], height=x.shape[2], channel=x.shape[3])(reshaped)\n","    reshaped_back = Reshape((x.shape[1], x.shape[2], x.shape[3]))(bottleneck)\n","    x = layers.UpSampling2D((2, 2), name=\"Up_after_bottleneck\")(reshaped_back)\n","\n","    x = gated_skip_connection(x, block5, mode=\"add\", gating=\"learnable\", name=\"decoder_network2_gated1\")\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"decoder_network2_conv2\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act2\")(x)\n","    x = layers.Conv2D(512, (3, 3), padding=\"same\", name=\"decoder_network2_conv3\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act3\")(x)\n","    x = layers.Dropout(0.3, name=\"Encoder_network1_dropout6\")(x)\n","    x = layers.BatchNormalization(name=\"decoder_network2_bn1\")(x)\n","    print(\"Input decoder x1: \", x.shape)\n","\n","    # Secondo Gated Skip Connection: Block 4\n","    x = layers.UpSampling2D((2, 2), name=\"decoder_network2_upsample1\")(x)\n","    x = gated_skip_connection(x, block4, mode=\"add\", gating=\"learnable\", name=\"decoder_network2_gated2\")\n","    x = layers.Conv2D(256, (3, 3), padding=\"same\", name=\"decoder_network2_conv4\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act4\")(x)\n","    x = layers.Conv2D(256, (3, 3), padding=\"same\", name=\"decoder_network2_conv5\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act5\")(x)\n","    x = layers.Dropout(0.3, name=\"Encoder_network1_dropout5\")(x)\n","    x = layers.BatchNormalization(name=\"decoder_network2_bn2\")(x)\n","    print(\"Input decoder x2: \", x.shape)\n","\n","    # Terzo Gated Skip Connection: Block 3\n","    x = layers.UpSampling2D((2, 2), name=\"decoder_network2_upsample2\")(x)\n","    x = gated_skip_connection(x, block3, mode=\"add\", gating=\"learnable\", name=\"decoder_network2_gated3\")\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", name=\"decoder_network2_conv6\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act6\")(x)\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", name=\"decoder_network2_conv7\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act7\")(x)\n","    x = layers.Dropout(0.3, name=\"Encoder_network1_dropout4\")(x)\n","    x = layers.BatchNormalization(name=\"decoder_network2_bn3\")(x)\n","    print(\"Input decoder x3: \", x.shape)\n","\n","    # Quarto Gated Skip Connection: Block 2\n","    x = layers.UpSampling2D((2, 2), name=\"decoder_network2_upsample3\")(x)\n","    x = gated_skip_connection(x, block2, mode=\"add\", gating=\"learnable\", name=\"decoder_network2_gated4\")\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", name=\"decoder_network2_conv8\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act8\")(x)\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", name=\"decoder_network2_conv9\")(x)\n","    x = layers.LeakyReLU(alpha=0.1, name=\"decoder_network2_act9\")(x)\n","    x = layers.Dropout(0.3, name=\"Encoder_network1_dropout3\")(x)\n","    x = layers.BatchNormalization(name=\"decoder_network2_bn4\")(x)\n","    print(\"Input decoder x4: \", x.shape)\n","\n","    # Risoluzione finale\n","    x = layers.UpSampling2D((2, 2), name=\"decoder_network1_upsample5\")(x)\n","    outputs = layers.Conv2D(num_classes, (1, 1), activation=\"softmax\", name=\"outputs1\")(x)\n","    print(\"output decoder: \", outputs.shape)\n","\n","    model = tfk.Model(inputs=inputs, outputs=outputs)\n","\n","    return model\n",""],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.203626Z","iopub.execute_input":"2024-12-09T14:05:58.203842Z","iopub.status.idle":"2024-12-09T14:05:58.226625Z","shell.execute_reply.started":"2024-12-09T14:05:58.203821Z","shell.execute_reply":"2024-12-09T14:05:58.226089Z"},"id":"oSOelKkX-ukk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["model = network1()\n","\n","# Print a detailed summary of the model with expanded nested layers and trainable parameters.\n","#model.summary(expand_nested=True, show_trainable=True)\n","\n","# Generate and display a graphical representation of the model architecture.\n","#tf.keras.utils.plot_model(model, show_trainable=True, expand_nested=True, dpi=70)"],"metadata":{"trusted":true,"id":"qeBm9-J-tEu4","execution":{"iopub.status.busy":"2024-12-09T14:05:58.227620Z","iopub.execute_input":"2024-12-09T14:05:58.227862Z","iopub.status.idle":"2024-12-09T14:05:58.811998Z","shell.execute_reply.started":"2024-12-09T14:05:58.227839Z","shell.execute_reply":"2024-12-09T14:05:58.811359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# MeanIntersectionOverUnion"],"metadata":{"id":"jPAVplQ3_iTB"}},{"cell_type":"code","source":["class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n","    def __init__(self, num_classes, labels_to_exclude=[0], name=\"mean_iou\", dtype=None, **kwargs):\n","        \"\"\"\n","        Aggiunto **kwargs per gestire parametri inattesi come `ignore_class`.\n","        \"\"\"\n","        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n","        if labels_to_exclude is None:\n","            labels_to_exclude = [0]\n","        self.labels_to_exclude = labels_to_exclude\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred = tf.math.argmax(y_pred, axis=-1)\n","        y_true = tf.reshape(y_true, [-1])\n","        y_pred = tf.reshape(y_pred, [-1])\n","\n","        for label in self.labels_to_exclude:\n","            mask = tf.not_equal(y_true, label)\n","            y_true = tf.boolean_mask(y_true, mask)\n","            y_pred = tf.boolean_mask(y_pred, mask)\n","\n","        return super().update_state(y_true, y_pred, sample_weight)\n","\n","# Registra la classe personalizzata\n","tf.keras.utils.get_custom_objects()[\"MeanIntersectionOverUnion\"] = MeanIntersectionOverUnion"],"metadata":{"trusted":true,"id":"wuZoUhEEtEu5","execution":{"iopub.status.busy":"2024-12-09T14:05:58.835734Z","iopub.execute_input":"2024-12-09T14:05:58.835954Z","iopub.status.idle":"2024-12-09T14:05:58.853220Z","shell.execute_reply.started":"2024-12-09T14:05:58.835933Z","shell.execute_reply":"2024-12-09T14:05:58.852757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Visualization Callback"],"metadata":{"id":"DRZmL1sFtEu6"}},{"cell_type":"code","source":["from matplotlib.colors import ListedColormap\n","\n","class VizCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, dataset, frequency=5, num_classes=2):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.frequency = frequency\n","        self.num_classes = num_classes\n","        self.dataset_iter = iter(dataset)  # Crea un iteratore per accedere ai dati\n","\n","        # Crea la mappa di colori per visualizzare le maschere\n","        # Aggiungi qui i colori per ogni classe\n","        colors = ['black', 'blue', 'green', 'yellow', 'red']  # Colori per 5 classi\n","        self.cmap = ListedColormap(colors)\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch % self.frequency == 0:  # Visualizza solo ogni \"frequency\" epochs\n","            try:\n","                # Estrai un batch di dati\n","                image, label = next(self.dataset_iter)\n","            except StopIteration:\n","                # Ricrea l'iteratore se i dati sono terminati\n","                self.dataset_iter = iter(self.dataset)\n","                image, label = next(self.dataset_iter)\n","\n","            # Prepara i dati per la predizione\n","            image = tf.expand_dims(image[0], 0)  # Estrai una sola immagine dal batch\n","            label = label[0]  # Etichetta corrispondente\n","            pred = self.model.predict(image, verbose=0)\n","            y_pred = tf.math.argmax(pred, axis=-1)\n","            y_pred = y_pred.numpy()\n","\n","            plt.figure(figsize=(16, 4))\n","\n","            # Immagine di input\n","            plt.subplot(1, 3, 1)\n","            plt.imshow(image[0], cmap=\"gray\")\n","            plt.title(\"Input Image\")\n","            plt.axis('off')\n","\n","            # Ground truth (maschera) con la mappa di colori\n","            plt.subplot(1, 3, 2)\n","            plt.imshow(label.numpy(), cmap=self.cmap, vmin=0, vmax=self.num_classes-1)\n","            plt.title(\"Ground Truth Mask\")\n","            plt.axis('off')\n","\n","            # Predizione con la mappa di colori\n","            plt.subplot(1, 3, 3)\n","            plt.imshow(y_pred[0], cmap=self.cmap, vmin=0, vmax=self.num_classes-1)\n","            plt.title(\"Predicted Mask\")\n","            plt.axis('off')\n","\n","            plt.tight_layout()\n","            plt.show()\n","            plt.close()\n"],"metadata":{"trusted":true,"id":"2FAx1JkCtEu6","execution":{"iopub.status.busy":"2024-12-09T14:05:58.854352Z","iopub.execute_input":"2024-12-09T14:05:58.854607Z","iopub.status.idle":"2024-12-09T14:05:58.870056Z","shell.execute_reply.started":"2024-12-09T14:05:58.854583Z","shell.execute_reply":"2024-12-09T14:05:58.869478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Losses"],"metadata":{"id":"s8p6QfgL-ukl"}},{"cell_type":"code","source":["from tensorflow.keras import backend as K\n","\n","def iou_loss(y_true, y_pred, smooth=1e-7):\n","    if y_true.shape[-1] != y_pred.shape[-1]:\n","        y_true = tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=tf.shape(y_pred)[-1])\n","\n","    # Calcolo di Intersection e Union\n","    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n","    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2]) - intersection\n","    iou = (intersection + smooth) / (union + smooth)\n","\n","    # Loss come complemento dell'IoU\n","    return 1 - tf.reduce_mean(iou)\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.871302Z","iopub.execute_input":"2024-12-09T14:05:58.871599Z","iopub.status.idle":"2024-12-09T14:05:58.887106Z","shell.execute_reply.started":"2024-12-09T14:05:58.871573Z","shell.execute_reply":"2024-12-09T14:05:58.886571Z"},"id":"xvJAzMhy-ukl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["labels_flat = y_train.reshape(-1)  # Appiattisci l'array per considerare tutti i pixel\n","\n","# Conta la frequenza di ogni classe\n","num_classes = 5  # Modifica in base al numero di classi nel tuo dataset\n","labels_flat = labels_flat.astype(int)  # Converte i dati in interi\n","\n","# Conta la frequenza di ogni classe\n","class_frequencies = np.bincount(labels_flat, minlength=num_classes)\n","\n","# Stampa le frequenze\n","print(f\"Frequenze delle classi: {class_frequencies}\")\n","\n","total_samples = np.sum(class_frequencies)\n","class_weights = total_samples / (len(class_frequencies) * class_frequencies)\n","print(f\"Pesi di classe: {class_weights}\")\n","\n","# Conversione in dizionario per TensorFlow\n","class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n","\n","# Funzione di perdita con pesi di classe\n","loss = tf.keras.losses.SparseCategoricalCrossentropy()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.888078Z","iopub.execute_input":"2024-12-09T14:05:58.888346Z","iopub.status.idle":"2024-12-09T14:05:58.983747Z","shell.execute_reply.started":"2024-12-09T14:05:58.888318Z","shell.execute_reply":"2024-12-09T14:05:58.983041Z"},"id":"S5lSQpm2-ukl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["class ComboLoss:\n","    def __init__(self, alpha=0.5, dice_smooth=1e-7, class_weights=None):\n","        self.alpha = alpha\n","        self.dice_smooth = dice_smooth\n","        self.class_weights = class_weights\n","\n","    def dice_loss(self, y_true, y_pred):\n","        y_true = tf.cast(tf.one_hot(tf.cast(y_true[..., 0], tf.int32), depth=y_pred.shape[-1]), tf.float32)\n","        intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n","        cardinality = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n","        dice = (2. * intersection + self.dice_smooth) / (cardinality + self.dice_smooth)\n","        return 1 - tf.reduce_mean(dice)\n","\n","    def weighted_loss(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.int32)  # Assicurati che i valori siano interi\n","        weights = tf.gather(class_weights, y_true)  # Recupera i pesi in base alle etichette\n","        weights = tf.cast(weights, tf.float32)  # Converte i pesi in float32\n","        unweighted_loss = loss(y_true, y_pred)\n","        weighted_loss = unweighted_loss * weights\n","        return tf.reduce_mean(weighted_loss)\n","\n","    def __call__(self, y_true, y_pred):\n","        wce = self.weighted_loss(y_true, y_pred)\n","        dice = self.dice_loss(y_true, y_pred)\n","        return self.alpha * wce + (1 - self.alpha) * dice\n","\n","    def get_config(self):\n","        return {\n","            \"alpha\": self.alpha,\n","            \"dice_smooth\": self.dice_smooth,\n","            \"class_weights\": self.class_weights\n","        }\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n","\n","combo_loss = ComboLoss(alpha=0.5, class_weights=class_weights)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:05:58.985031Z","iopub.execute_input":"2024-12-09T14:05:58.985418Z","iopub.status.idle":"2024-12-09T14:05:58.993567Z","shell.execute_reply.started":"2024-12-09T14:05:58.985385Z","shell.execute_reply":"2024-12-09T14:05:58.992941Z"},"id":"e5yYbqwf-ukl"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Early Stopping Callback"],"metadata":{"id":"cCUHw_1DtEu6"}},{"cell_type":"code","source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy',\n","    mode='max',\n","    patience=patience,\n","    restore_best_weights=True\n",")\n","\n","viz_callback = VizCallback(val_dataset, frequency=5, num_classes=5)\n"],"metadata":{"trusted":true,"id":"swRRN3tetEu6","execution":{"iopub.status.busy":"2024-12-09T14:05:58.998257Z","iopub.execute_input":"2024-12-09T14:05:58.998561Z","iopub.status.idle":"2024-12-09T14:05:59.260533Z","shell.execute_reply.started":"2024-12-09T14:05:58.998537Z","shell.execute_reply":"2024-12-09T14:05:59.259914Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Compile"],"metadata":{"id":"-Mj-x4FytEu7"}},{"cell_type":"code","source":["model.compile(\n","    loss=combo_loss,\n","    optimizer=tf.keras.optimizers.AdamW(0.001),\n","    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0])]\n",")"],"metadata":{"trusted":true,"id":"y1wYlotdtEu7","execution":{"iopub.status.busy":"2024-12-09T14:05:59.261552Z","iopub.execute_input":"2024-12-09T14:05:59.261814Z","iopub.status.idle":"2024-12-09T14:05:59.274731Z","shell.execute_reply.started":"2024-12-09T14:05:59.261789Z","shell.execute_reply":"2024-12-09T14:05:59.274124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"gWrzNJAvtEu7"}},{"cell_type":"code","source":["history = model.fit(\n","    train_dataset,\n","    epochs=epoch,\n","    validation_data=val_dataset,\n","    callbacks=[early_stopping, viz_callback],\n","    verbose=1\n",").history"],"metadata":{"trusted":true,"id":"3gkzAWJdtEu7","execution":{"iopub.status.busy":"2024-12-09T14:05:59.275700Z","iopub.execute_input":"2024-12-09T14:05:59.275975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Calculate and print the final validation accuracy\n","final_val_meanIoU = round(max(history['val_mean_iou'])* 100, 2)\n","print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n","\n","final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n","print(f'Final validation Accuracy: {final_val_accuracy}%')\n","\n","\n","# Definisci il percorso di salvataggio nella directory di lavoro\n","model.save(\"/kaggle/working/model_.keras\")\n","\n","# Delete the model to free up resources\n","del model"],"metadata":{"trusted":true,"id":"QRY4-R0E-ukq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"NQ9ADLjOtEu8"}},{"cell_type":"code","source":["model_filename = \"model_.keras\"\n","X_test = training_data[\"test_set\"]\n","model = tf.keras.models.load_model(\n","    \"/kaggle/working/model_.keras\",\n","    custom_objects={\n","        'ComboLoss': ComboLoss,\n","        'TransformerBottleneck': TransformerBottleneck\n","    }\n",")\n","\n","preds = model.predict(X_test/255)\n","preds = np.argmax(preds, axis=-1)\n","print(f\"Predictions shape: {preds.shape}\")"],"metadata":{"trusted":true,"id":"ERj1cbAjtEu8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def y_to_df(y) -> pd.DataFrame:\n","    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n","    n_samples = len(y)\n","    y_flat = y.reshape(n_samples, -1)\n","    df = pd.DataFrame(y_flat)\n","    df[\"id\"] = np.arange(n_samples)\n","    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n","    return df[cols]"],"metadata":{"trusted":true,"id":"B2pHkatXtEu8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Create and download the csv submission file\n","timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n","submission_filename = f\"submission_{timestep_str}.csv\"\n","submission_df = y_to_df(preds)\n","submission_df.to_csv(submission_filename, index=False)"],"metadata":{"trusted":true,"id":"O_47nUq8tEu9"},"outputs":[],"execution_count":null}]}